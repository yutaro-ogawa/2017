title

SARSA法およびモンテカルロ法の実装


動画


強化学習の代表的な手法である「SARSA法」と「モンテカルロ法」の解説・および実装を行います


強化学習する対象には、強化学習のHello World！的存在である「CartPole」を使用します。



概要
強化学習の代表的な手法であるSARSA法、モンテカルロ法の2通りを実装しました。
※ディープラーニングではありません。古典的？な強化学習です


どちらも150行程度の短いプログラムです。

外部の強化学習ライブラリなどを一切使用せず、自力で組んでいます。



対象者：Qiitaの強化学習の記事●●を読み、次は実装方法を知りたい方
強化学習に興味はあるが、実装方法が思い浮かばない人


得られるもの：
SARSA法およびモンテカルロ法を用いた「シンプルミニマムな強化学習の実装方法」を知ることができます。





本記事に入る前に
以下の記事で、強化学習、Q学習、SARSA、モンテカルロ法について概要をつかんでください。

●

その後、こちらの記事で、棒立て問題を制御する「Open AI gymのCartPole」と「Q学習の実装方法」をご覧ください。


●



それではまずSARSA法について説明します。


# SARSA法

SARSAとは、
State, Action, Reward, State(next), Action(next)の頭文字をとった手法です。


Q学習とはQ関数の更新方法が少し異なるだけであり、Q学習の実装を理解していれば簡単に理解することができます。


まずQ学習でのQ関数、Q(State, Action)の更新について説明します。

Q学習ではQ(State, Action)が
Reward + γ*MAX[Q(State(next), Action(next))]

に近づくように更新しました。
（γは時間割引率）


そして実際に次の行動Action(next)はε-greedy法にしたがって決定しました。
※ε-greedy法
報酬が最大になると期待される行動を選択するが、ときおりランダムに行動して、探索と最適化のバランスをとる手法

そのため、Q学習の場合はQ関数の更新に使用したAction(next)と次の時刻での実際の行動Action(next)が異なる可能性がありました。



次に、SARSA法でのQ関数、Q(State, Action)の更新について説明します。


ーーーーー

まずQ学習でもSARSAでも、次の行動Action(next)はε-greedy法にしたがって決定します。


Q学習ではQ(State, Action)が
Reward + γ*MAX[Q(State(next), Action(next))]

に近づくように更新しました。
（γは時間割引率）


そして実際に次の行動Action(next)はε-greedy法にしたがって決定しました。
※ε-greedy法
報酬が最大になると期待される行動を選択するが、ときおりランダムに行動して、探索と最適化のバランスをとる手法

そのため、Q学習の場合はQ関数の更新に使用したAction(next)と次の時刻での実際の行動Action(next)が異なる可能性がありました。








