title

SARSA法およびモンテカルロ法の実装


動画


強化学習の代表的な手法である「SARSA法」と「モンテカルロ法」の実装コードの紹介と解説を行います


学習する対象には、強化学習のHello World！的存在である「CartPole」を使用します。



概要
強化学習の代表的な手法であるSARSA法、モンテカルロ法の2通りを実装・解説しました。
※ディープラーニングは使用しません。古典的？な強化学習です


どちらも150行程度の短いプログラムです。
外部の強化学習ライブラリなどを使用せず、自力で組んでいます。
コメント多めです。
保守性よりも、初学者が分かりやすいように、コードを書いています。


対象者：
Qiitaの強化学習の記事●●を読み、次は実装方法を知りたい方
強化学習に興味はあるが、実装方法が思い浮かばない人
SARSAやモンテカルロ法を、実装してみたい人
難しい数式並べられるよりも、実際のコードを見たほうが理解が進む人


得られるもの：
SARSA法およびモンテカルロ法を用いた「シンプルミニマムな強化学習の実装例」を知ることができます。




本記事に入る前に
以下の記事で、強化学習、Q学習について概要をつかんでください。
SARSA、モンテカルロ法の説明もあります。

●



その後、こちらの記事で、棒立て問題を制御する「Open AI gymのCartPoleの使い方」と「Q学習の実装方法」をご覧ください。

●



それではまずSARSA法について説明します。


# SARSA法

SARSAとは、
State, Action, Reward, State(next), Action(next)の頭文字をとった手法です。


SARSA法とQ学習と比べると、その違いはQ関数の更新方法が少し異なるだけです。

Q学習の実装を理解していれば、SARSAは簡単に理解することができます。


まずおさらいとして、Q学習での Q関数=Q(State, Action) の更新について説明します。

Q学習ではQ(State, Action)が
Reward + γ*MAX[Q(State(next), Action(next))]

に近づくように更新しました。
（γは時間割引率）


そしてそのあとで、実際に行う次の行動Action(next)を、ε-greedy法にしたがって決定しました。

※ε-greedy法
報酬が最大になると期待される行動を選択するが、ときおりランダムに行動して、探索と最適化のバランスをとる手法


そのため、Q学習の場合はQ関数の更新に使用したAction(next)と、次の時刻での実際の行動Action(next)が異なる可能性がありました。


次に、SARSA法でのQ関数、Q(State, Action)の更新について説明します。

SARSAでは次の時刻の行動Action(next)を、Q関数の更新より前に決定してしまいます。

SARSAでも、次の行動Action(next)はε-greedy法にしたがって決定します。

そしてQ関数の更新を
Q(State, Action)が
Reward + γ*Q(State(next), Action(next))

に近づくように更新します。


つまりSARSAでは、実際に行う次の行動のQ値を使用して、Q関数を更新します。

ステップ数が進み、ε-greedy法がほとんど探索を行わず最適行動のみを行う場合には、Q学習もSARSAも同じとなります。


一方で試行の初期で探索が多い場面では、SARSAは実際の行動を反映し、Q学習は期待される最大のものを使用するという特徴があります。

正直私には、Q学習とSARSAの使い分けがきちんとわからないのですが、このような方法もあること、
そしてその実装方法を知っておくのは良いと思います。


SARSAでの学習は約1000試行で収束し、以下のような結果になります。


※動画



実装コードは以下の通りです。


※実装コード



Q学習のコード
●

に比べて、
# [3]Qテーブルを更新する関数(SARSA)
のQ関数の更新部分が異なっています。

Q学習では次の行動を決めておく必要はありませんでしたが、SARSAでは次の行動Action(next)を決め、Q関数の更新に使用しています。



それでは次にモンテカルロ法による強化学習について説明します。


# モンテカルロ法による強化学習

モンテカルロ法による強化学習は、Q学習やSARSAとは少し毛色が異なります。

Q関数を使用するという点は同じです。

一方で、モンテカルロ法では、各ステップごとにQ関数を更新しないという特徴があります。

その代わりに、試行が終了した時点でQ関数を全ステップ分、一気に更新します。

そのため、試行終了までの、各ステップでの（状態s、行動a、得た報酬r)をすべて記憶しておきます。

それではモンテカルロ法でのQ関数の更新を説明します。

例えば、ステップ = T、でCartPoleが倒れたとします。
そのときの状態と行動はs(T)とa(T)と表されます。
また得た報酬はr(T)となります。


するとQ関数の更新は、

Q(s_T, a_T)がr(T)に近づくように更新します。

その後、現在時刻以降で得られた報酬をtotal_reward_tとして表すことにします。

時刻Tでは
total_reward_t = r(T)
です。

total_reward_tは、次のステップ以降で順繰りに使用します。



次にステップを一つ前に戻りT-1を更新します。

そこでの更新は、

Q(s_{T-1}, a_{T-1})が、
r(T-1)+γ*r(T)に近づくように更新します。

これは先ほど定義したtotal_reward_tを使用すれば、

r(T-1)+γ*total_reward_t

と表されます。

先に
total_reward_t　←　γ*total_reward_t
と更新しておけば、

r(T-1)+total_reward_t

となります。

そのため、Q(s_{T-1}, a_{T-1})が
r(T-1)+total_reward_t
に近づくように更新することになります。

最後に、
total_reward_t　←　r(T-1)+total_reward_t
と更新しておきます。


次にさらにステップを一つさかのぼり、t=T-2を考えます。

そこでの更新は、

Q(s_{T-2}, a_{T-2})が、

r(T-2)+γ*r(T-1)+γ*γ*r(T)

に近づくように更新します。


先に
total_reward_t　←　γ*total_reward_t
と更新しておけば、

r(T-2)+total_reward_t

となります。

そのため、Q(s_{T-2}, a_{T-2})が
r(T-2)+total_reward_t
に近づくように更新することになります。

最後に、
total_reward_t　←　r(T-2)+total_reward_t
と更新しておきます。


次はt=T-3を行います。

total_reward_t　←　γ*total_reward_t
と更新して、

Q(s_{T-3}, a_{T-3})が
r(T-3)+total_reward_t
に近づくように更新することになります。


ずっとこの繰り返しです。

このようにQ関数を、試行の最後のステップから時刻0まで順番に更新していきます。


モンテカルロ法には試行の途中でQ関数を更新できないという欠点があります。
一方で2つの利点があります

1つ目の利点は、実際に得た報酬でQ関数を更新できるという点です。

Q学習やSARSAの場合にはQ(s_t, a_t)の更新にQ(s_{t+1}, a_{t+1})という不確かなQ関数を使用していました。

モンテカルロ法は実際の報酬を使用するので、学習初期でQ関数が確かな方向に学習しやすいという利点につながります。

2つ目の利点が、「試行の途中で報酬がもらえない」、もしくは「報酬をうまく規定しづらいタスク」の学習に対応しやすいということです。

例えば囲碁や将棋などでは、途中の報酬を決めるのが難しいです。
（飛車を取られても、局面が有利になることもあるかもしれないですし）

その場合、最終的に勝った、負けた、だけが、信頼ある報酬と考えることができます。

このような最終的結果からQ関数を学習することができます。

その他に、私がモンテカルロ法を知ってはじめに疑問だったのが「行動の決め方」です。
ですが、これは通常のε-greedy法で大丈夫です。


以上の点を踏まえて、結果とコードを紹介します。
だいたい1000試行以下で学習が収束します。



※動画


※実装コード




以上、強化学習のSARSAとモンテカルロ法を紹介しました。

また次回も強化学習の実装を紹介する予定ですので、よろしくお願いします。


