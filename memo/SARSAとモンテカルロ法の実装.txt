title

SARSA法およびモンテカルロ法の実装


動画


強化学習の代表的な手法である「SARSA法」と「モンテカルロ法」の解説・および実装を行います


強化学習する対象には、強化学習のHello World！的存在である「CartPole」を使用します。



概要
強化学習の代表的な手法であるSARSA法、モンテカルロ法の2通りを実装しました。
※ディープラーニングではありません。古典的？な強化学習です


どちらも150行程度の短いプログラムです。

外部の強化学習ライブラリなどを一切使用せず、自力で組んでいます。



対象者：Qiitaの強化学習の記事●●を読み、次は実装方法を知りたい方
強化学習に興味はあるが、実装方法が思い浮かばない人


得られるもの：
SARSA法およびモンテカルロ法を用いた「シンプルミニマムな強化学習の実装方法」を知ることができます。





本記事に入る前に
以下の記事で、強化学習、Q学習、SARSA、モンテカルロ法について概要をつかんでください。

●

その後、こちらの記事で、棒立て問題を制御する「Open AI gymのCartPole」と「Q学習の実装方法」をご覧ください。


●



それではまずSARSA法について説明します。


# SARSA法

SARSAとは、
State, Action, Reward, State(next), Action(next)の頭文字をとった手法です。


SARSAは、Q学習と比べると、Q関数の更新方法が少し異なるだけです。
Q学習の実装を理解していれば簡単に理解することができます。


まずQ学習でのQ関数、Q(State, Action)の更新について説明します。

Q学習ではQ(State, Action)が
Reward + γ*MAX[Q(State(next), Action(next))]

に近づくように更新しました。
（γは時間割引率）


そして実際に行う次の行動Action(next)は、ε-greedy法にしたがって決定しました。

※ε-greedy法
報酬が最大になると期待される行動を選択するが、ときおりランダムに行動して、探索と最適化のバランスをとる手法

そのため、Q学習の場合はQ関数の更新に使用したAction(next)と、次の時刻での実際の行動Action(next)が異なる可能性がありました。



次に、SARSA法でのQ関数、Q(State, Action)の更新について説明します。
SARSAでは次の時刻の行動を先に決定してしまいます。

SARSAでも、次の行動Action(next)はε-greedy法にしたがって決定します。

そしてQ関数の更新を
Q(State, Action)が
Reward + γ*Q(State(next), Action(next))

に近づくように更新します。


つまり、SARSAでは実際に行う次の行動を使用して、Q関数を更新します。

ステップ数が進み、ε-greedy法がほとんど探索でなく最適行動を行う場合には、Q学習もSARSAも同じことをしています。
一方で探索の初期では、SARSAは実際の行動を反映し、Q学習は期待される最大のものを使用するという特徴があります。

正直私には、Q学習とSARSAの使い分けがきちんとわからないのですが、このような方法もあること、
そしてその実装方法を知っておくのは良いと思います。


SARSAでの学習は約1000試行で収束し、以下のような結果になります。


※動画



実装コードは以下の通りです。


※実装コード



Q学習のコード
●

に比べて、
# [3]Qテーブルを更新する関数(SARSA)
のQ関数の更新部分が異なっています。

Q学習では次の行動を決めておく必要はありませんでしたが、SARSAでは次の行動Action(next)を決め、Q関数の更新に使用しています。


それでは次にモンテカルロ法による強化学習について説明します。


# モンテカルロ法による強化学習

モンテカルロ法による強化学習は、Q学習やSARSAとは少し毛色が異なります。

Q関数を使用するという点は同じです。

一方で、モンテカルロ法ではステップごとにQ関数を更新しないという特徴があります。

その代わりに、試行が終了した時点でQ関数を全ステップ分一気に更新します。


例えば、ステップ = T、でCartPoleが倒れたとします。
そしてそのときまでの報酬の合計がRであったとします。

すると、状態s(T)、行動a(T)で














