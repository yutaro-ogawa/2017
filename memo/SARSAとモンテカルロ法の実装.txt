title

SARSA法およびモンテカルロ法の実装


動画


強化学習の代表的な手法である「SARSA法」と「モンテカルロ法」の解説・および実装を行います


強化学習する対象には、強化学習のHello World！的存在である「CartPole」を使用します。



概要
強化学習の代表的な手法であるSARSA法、モンテカルロ法の2通りを実装しました。
※ディープラーニングではありません。古典的？な強化学習です


どちらも150行程度の短いプログラムです。

外部の強化学習ライブラリなどを一切使用せず、自力で組んでいます。



対象者：Qiitaの強化学習の記事●●を読み、次は実装方法を知りたい方
強化学習に興味はあるが、実装方法が思い浮かばない人


得られるもの：
SARSA法およびモンテカルロ法を用いた「シンプルミニマムな強化学習の実装方法」を知ることができます。





本記事に入る前に
以下の記事で、強化学習、Q学習、SARSA、モンテカルロ法について概要をつかんでください。

●

その後、こちらの記事で、棒立て問題を制御する「Open AI gymのCartPole」と「Q学習の実装方法」をご覧ください。


●



それではまずSARSA法について説明します。


# SARSA法

SARSAとは、
State, Action, Reward, State(next), Action(next)の頭文字をとった手法です。


SARSAは、Q学習と比べると、Q関数の更新方法が少し異なるだけです。
Q学習の実装を理解していれば簡単に理解することができます。


まずQ学習でのQ関数、Q(State, Action)の更新について説明します。

Q学習ではQ(State, Action)が
Reward + γ*MAX[Q(State(next), Action(next))]

に近づくように更新しました。
（γは時間割引率）


そして実際に行う次の行動Action(next)は、ε-greedy法にしたがって決定しました。

※ε-greedy法
報酬が最大になると期待される行動を選択するが、ときおりランダムに行動して、探索と最適化のバランスをとる手法

そのため、Q学習の場合はQ関数の更新に使用したAction(next)と、次の時刻での実際の行動Action(next)が異なる可能性がありました。



次に、SARSA法でのQ関数、Q(State, Action)の更新について説明します。
SARSAでは次の時刻の行動を先に決定してしまいます。

SARSAでも、次の行動Action(next)はε-greedy法にしたがって決定します。

そしてQ関数の更新を
Q(State, Action)が
Reward + γ*Q(State(next), Action(next))

に近づくように更新します。


つまり、SARSAでは実際に行う次の行動を使用して、Q関数を更新します。

ステップ数が進み、ε-greedy法がほとんど探索でなく最適行動を行う場合には、Q学習もSARSAも同じことをしています。
一方で探索の初期では、SARSAは実際の行動を反映し、Q学習は期待される最大のものを使用するという特徴があります。

正直私には、Q学習とSARSAの使い分けがきちんとわからないのですが、このような方法もあること、
そしてその実装方法を知っておくのは良いと思います。


SARSAでの学習は約1000試行で収束し、以下のような結果になります。


※動画



実装コードは以下の通りです。


※実装コード



Q学習のコード
●

に比べて、
# [3]Qテーブルを更新する関数(SARSA)
のQ関数の更新部分が異なっています。

Q学習では次の行動を決めておく必要はありませんでしたが、SARSAでは次の行動Action(next)を決め、Q関数の更新に使用しています。


それでは次にモンテカルロ法による強化学習について説明します。


# モンテカルロ法による強化学習

モンテカルロ法による強化学習は、Q学習やSARSAとは少し毛色が異なります。

Q関数を使用するという点は同じです。

一方で、モンテカルロ法ではステップごとにQ関数を更新しないという特徴があります。

その代わりに、試行が終了した時点でQ関数を全ステップ分一気に更新します。

そのため、試行終了までの、各ステップでの状態と行動、得た報酬をすべて記憶しておきます。

例えば、ステップ = T、でCartPoleが倒れたとします。
そのときの状態と行動はs(T)とa(T)と表されます。
また得た報酬はr(T)となります。


するとQ関数の更新は、

Q(s_T, a_T)がr(T)に近づくように更新します。

ここで時刻T以降で得られた報酬をtotal_reward_tとして表すことにします。

時刻Tでは
total_reward_t = r(T)
です。

total_reward_tは次以降のステップで順繰りに使用します。



次にステップを一つ前に戻ります。

そこでの更新は、

Q(s_{T-1}, a_{T-1})が、
r(T-1)+γ*r(T)に近づくように更新します。

これは先ほどのtotal_reward_tを使用すれば、

r(T-1)+γ*total_reward_t

と表されます。

先に
total_reward_t　←　γ*total_reward_t
と更新しておけば、

r(T-1)+total_reward_t

となります。

そのため、Q(s_{T-1}, a_{T-1})が
r(T-1)+total_reward_t
に近づくように更新することになります。

最後に、
total_reward_t　←　r(T-1)+total_reward_t
と更新しておきます。


次にさらにステップを一つさかのぼり、t=T-2を考えます。

そこでの更新は、

Q(s_{T-2}, a_{T-2})が、

r(T-2)+γ*r(T-1)+γ*γ*r(T)

に近づくように更新します。


先に
total_reward_t　←　γ*total_reward_t
と更新しておけば、

r(T-2)+total_reward_t

となります。

そのため、Q(s_{T-2}, a_{T-2})が
r(T-2)+total_reward_t
に近づくように更新することになります。

最後に、
total_reward_t　←　r(T-2)+total_reward_t
と更新しておきます。


次はt=T-3を行います。
内容はこれまでと同じことの繰り返しです。

このようにQ関数を試行の最後のステップから時刻0まで順番に更新していきます。


モンテカルロ法はこのように試行の途中でQ関数を更新できないという欠点がありますが、



















